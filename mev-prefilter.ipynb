{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "import argparse\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# API Configuration\n",
    "AIRTABLE_TOKEN = os.getenv('REACT_APP_AIRTABLE_TOKEN')\n",
    "AIRTABLE_BASE_URL = 'https://api.airtable.com/v0/appZWDvjvDmVnOici'\n",
    "TABLE_NAME = 'tblcXnFAf0IEvAQA6'\n",
    "TARGET_VIEW_ID = 'viwF2Xc24CGNO7u5C'  # MEV Prefilter viwF2Xc24CGNO7u5C, Targeted: viwx6juMBenBuY6hs\n",
    "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "\n",
    "# Contract Explorer API Configuration (for fetching transaction data)\n",
    "CONTRACT_API_BASE_URL = os.getenv('CONTRACT_API_BASE_URL', 'https://api.tenderly.co')\n",
    "\n",
    "# HyperSync API Configuration\n",
    "HYPERSYNC_API_URL = os.getenv('HYPERSYNC_API_URL')\n",
    "HYPERSYNC_API_KEY = os.getenv('HYPERSYNC_API_KEY')\n",
    "\n",
    "# API Headers\n",
    "AIRTABLE_HEADERS = {\n",
    "    'Authorization': f'Bearer {AIRTABLE_TOKEN}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "GITHUB_HEADERS = {\n",
    "    'Authorization': f'token {GITHUB_TOKEN}',\n",
    "    'Accept': 'application/vnd.github.v3+json'\n",
    "}\n",
    "\n",
    "CONTRACT_API_HEADERS = {\n",
    "    'Content-Type': 'application/json'\n",
    "    # Add any additional headers needed for the contract API\n",
    "}\n",
    "\n",
    "HYPERSYNC_HEADERS = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {HYPERSYNC_API_KEY}'\n",
    "}\n",
    "\n",
    "# List of GitHub repositories to exclude from search results\n",
    "EXCLUDED_REPOS = [\n",
    "    \"HelayLiu/utils_download\",\n",
    "    \"KeystoneHQ/Smart-Contract-Metadata-Registry\",\n",
    "    \"tangtj/\"\n",
    "]\n",
    "\n",
    "HYPERSYNC_CHAIN_CONFIG = {\n",
    "    # Chain ID: {\"url\": \"...\", \"supports_traces\": True/False}\n",
    "    1: {\"url\": \"https://eth.hypersync.xyz\", \"supports_traces\": True},          # Ethereum Mainnet\n",
    "    10: {\"url\": \"https://optimism.hypersync.xyz\", \"supports_traces\": False},     # Optimism\n",
    "    56: {\"url\": \"https://bsc.hypersync.xyz\", \"supports_traces\": False},         # Bsc (Update SUPPORTED_CHAINS if needed)\n",
    "    100: {\"url\": \"https://gnosis.hypersync.xyz\", \"supports_traces\": False},      # Gnosis (Add to SUPPORTED_CHAINS if needed)\n",
    "    137: {\"url\": \"https://polygon.hypersync.xyz\", \"supports_traces\": False},     # Polygon\n",
    "    250: {\"url\": \"https://fantom.hypersync.xyz\", \"supports_traces\": False},     # Fantom (Add to SUPPORTED_CHAINS if needed)\n",
    "    8453: {\"url\": \"https://base.hypersync.xyz\", \"supports_traces\": False},       # Base\n",
    "    42161: {\"url\": \"https://arbitrum.hypersync.xyz\", \"supports_traces\": False},  # Arbitrum\n",
    "    43114: {\"url\": \"https://avalanche.hypersync.xyz\", \"supports_traces\": False},# Avalanche (Add to SUPPORTED_CHAINS if needed)\n",
    "    # Add other mainnets or testnets as required, checking trace support from the list\n",
    "}\n",
    "\n",
    "# List of supported blockchain chains and their IDs\n",
    "SUPPORTED_CHAINS = {\n",
    "    \"base\": 8453,\n",
    "    \"ethereum\": 1,\n",
    "    \"optimism\": 10,\n",
    "    \"arbitrum\": 42161,\n",
    "    \"polygon\": 137\n",
    "}\n",
    "\n",
    "# Check for missing configuration\n",
    "if not all([AIRTABLE_TOKEN, GITHUB_TOKEN]):\n",
    "    print(\"WARNING: Missing AIRTABLE_TOKEN or GITHUB_TOKEN environment variable!\")\n",
    "if CONTRACT_API_BASE_URL == 'https://api.example.com':\n",
    "    print(\"WARNING: CONTRACT_API_BASE_URL is set to the placeholder. Please update it.\")\n",
    "if not HYPERSYNC_API_URL or not HYPERSYNC_API_KEY:\n",
    "    print(\"WARNING: Missing HYPERSYNC_API_URL or HYPERSYNC_API_KEY environment variable!\")\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AirtableAPI class defined.\n"
     ]
    }
   ],
   "source": [
    "class AirtableAPI:\n",
    "    \"\"\"Handle all Airtable API interactions.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def save_no_github_contracts(contract_address: str, record_id: str, origin_key: str = None):\n",
    "        \"\"\"Save contracts with no GitHub repositories to a JSON file.\"\"\"\n",
    "        filename = 'no_github_contracts.json'\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        # Create the data structure for this contract\n",
    "        contract_data = {\n",
    "            'contract_address': contract_address,\n",
    "            'record_id': record_id,\n",
    "            'origin_key': origin_key,\n",
    "            'timestamp': timestamp\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Load existing data if file exists\n",
    "            existing_data = []\n",
    "            if os.path.exists(filename):\n",
    "                with open(filename, 'r') as f:\n",
    "                    try:\n",
    "                        existing_data = json.load(f)\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Warning: {filename} contains invalid JSON. Initializing as empty list.\")\n",
    "                        existing_data = []\n",
    "            \n",
    "            # Ensure existing_data is a list\n",
    "            if not isinstance(existing_data, list):\n",
    "                print(f\"Warning: {filename} does not contain a list. Initializing as empty list.\")\n",
    "                existing_data = []\n",
    "                \n",
    "            # Add new contract data\n",
    "            existing_data.append(contract_data)\n",
    "            \n",
    "            # Save updated data\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(existing_data, f, indent=2)\n",
    "                \n",
    "            print(f\"Saved contract {contract_address} to {filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to {filename}: {str(e)}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_view_structure(view_id: str = TARGET_VIEW_ID) -> Dict:\n",
    "        \"\"\"Fetch basic information about a view to provide context.\"\"\"\n",
    "        url = f\"{AIRTABLE_BASE_URL}/{TABLE_NAME}?view={view_id}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=AIRTABLE_HEADERS, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract basic view information\n",
    "            records = data.get('records', [])\n",
    "            \n",
    "            # Get field names from the first record\n",
    "            field_names = []\n",
    "            if records and len(records) > 0:\n",
    "                field_names = list(records[0].get('fields', {}).keys())\n",
    "            \n",
    "            # Create a simple structure summary\n",
    "            view_structure = {\n",
    "                'view_id': view_id,\n",
    "                'total_records_in_response': len(records),\n",
    "                'field_names': field_names,\n",
    "                'sample_record': records[0].get('fields') if records else None\n",
    "            }\n",
    "            \n",
    "            return view_structure\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching view structure: {str(e)}\")\n",
    "            return {'error': str(e)}\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_all_unprocessed_contracts() -> List[Dict]:\n",
    "        \"\"\"Fetch all unprocessed contracts from the target view regardless of origin_key.\"\"\"\n",
    "        all_records = []\n",
    "\n",
    "        # Filter for records where repo_count field doesn't exist or is null\n",
    "        filter_formula = \"NOT({repo_count})\"\n",
    "        base_url = (\n",
    "            f\"{AIRTABLE_BASE_URL}/{TABLE_NAME}\"\n",
    "            f\"?view={TARGET_VIEW_ID}\"\n",
    "            f\"&filterByFormula={urllib.parse.quote(filter_formula)}\"\n",
    "            f\"&fields[]=address&fields[]=origin_key&fields[]=repo_count&fields[]=chain\"\n",
    "        )\n",
    "\n",
    "        offset = None\n",
    "        page = 1\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                # Add offset for pagination if it exists\n",
    "                url = f\"{base_url}&offset={offset}\" if offset else base_url\n",
    "                print(f\"Fetching page {page}: {url}\")\n",
    "                response = requests.get(url, headers=AIRTABLE_HEADERS, timeout=30)\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    print(f\"Airtable API Error ({response.status_code}): {response.text}\")\n",
    "                response.raise_for_status()\n",
    "\n",
    "                data = response.json()\n",
    "                records = data.get('records', [])\n",
    "                all_records.extend(records)\n",
    "\n",
    "                print(f\"Page {page}: Fetched {len(records)} records. Total: {len(all_records)}\")\n",
    "\n",
    "                # Check for more pages\n",
    "                offset = data.get('offset')\n",
    "                if not offset:\n",
    "                    break\n",
    "\n",
    "                page += 1\n",
    "                time.sleep(0.2)  # Respect rate limits\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching page {page}: {str(e)}\")\n",
    "                break\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from Airtable on page {page}: {str(e)}\")\n",
    "                print(f\"Response text: {response.text}\")\n",
    "                break\n",
    "\n",
    "        print(f\"Total unprocessed records (NULL repo_count): {len(all_records)}\")\n",
    "        return all_records\n",
    "\n",
    "    @staticmethod\n",
    "    def update_record(record_id: str, github_found: bool, repo_count: int, \n",
    "                     transaction_data: Optional[Dict] = None) -> bool:\n",
    "        \"\"\"Update an Airtable record with GitHub search results and optional transaction data.\"\"\"\n",
    "        url = f\"{AIRTABLE_BASE_URL}/{TABLE_NAME}/{record_id}\"\n",
    "\n",
    "        # Create the fields dictionary with github_found and repo_count\n",
    "        fields = {\n",
    "            \"github_found\": github_found,\n",
    "            \"repo_count\": repo_count\n",
    "        }\n",
    "        \n",
    "        # Add transaction data if provided\n",
    "        if transaction_data:\n",
    "            try:\n",
    "                fields[\"transaction_data\"] = json.dumps(transaction_data)\n",
    "            except TypeError as e:\n",
    "                print(f\"Error serializing transaction_data for record {record_id}: {str(e)}\")\n",
    "                return False\n",
    "\n",
    "        request_body = {\n",
    "            \"fields\": fields,\n",
    "            \"typecast\": True\n",
    "        }\n",
    "\n",
    "        max_retries = 3\n",
    "        current_retry = 0\n",
    "\n",
    "        while current_retry < max_retries:\n",
    "            try:\n",
    "                response = requests.patch(\n",
    "                    url,\n",
    "                    headers=AIRTABLE_HEADERS,\n",
    "                    json=request_body,\n",
    "                    verify=True,\n",
    "                    timeout=30\n",
    "                )\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    print(f\"Airtable Update Error ({response.status_code}) for record {record_id}: {response.text}\")\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                status = \"FOUND\" if github_found else \"NOT FOUND\"\n",
    "                print(f\"Updated record {record_id} - GitHub repositories: {status} (count: {repo_count})\")\n",
    "                if transaction_data:\n",
    "                    print(f\"Added transaction data for {record_id} with {len(transaction_data.get('transactions', []))} transactions\")\n",
    "                return True\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                current_retry += 1\n",
    "                print(f\"Error updating record {record_id} (Attempt {current_retry}/{max_retries}): {str(e)}\")\n",
    "\n",
    "                if current_retry < max_retries:\n",
    "                    time.sleep(2 ** current_retry)  # Exponential backoff\n",
    "                    continue\n",
    "\n",
    "                return False\n",
    "\n",
    "print(\"AirtableAPI class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHubAPI class defined.\n"
     ]
    }
   ],
   "source": [
    "class GitHubAPI:\n",
    "    \"\"\"Handle all GitHub API interactions.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def search_contract(address: str) -> Tuple[bool, int]:\n",
    "        \"\"\"Search GitHub for a contract address and return whether any results were found.\"\"\"\n",
    "        url = \"https://api.github.com/search/code\"\n",
    "        params = {\n",
    "            \"q\": address,\n",
    "            \"per_page\": 10  # Use larger page size to minimize pagination\n",
    "        }\n",
    "\n",
    "        max_retries = 3\n",
    "        current_retry = 0\n",
    "\n",
    "        while current_retry < max_retries:\n",
    "            try:\n",
    "                response = requests.get(\n",
    "                    url,\n",
    "                    headers=GITHUB_HEADERS,\n",
    "                    params=params,\n",
    "                    verify=True,\n",
    "                    timeout=30\n",
    "                )\n",
    "\n",
    "                # Handle rate limiting\n",
    "                if response.status_code == 403:\n",
    "                    remaining = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "                    reset_time = int(response.headers.get('X-RateLimit-Reset', time.time()))\n",
    "                    if remaining == 0:\n",
    "                        sleep_time = max(1, reset_time - int(time.time())) + 5  # Add buffer\n",
    "                        print(f\"GitHub rate limit exceeded. Waiting {sleep_time} seconds...\")\n",
    "                        time.sleep(sleep_time)\n",
    "                        continue\n",
    "\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"GitHub API Error ({response.status_code}): {response.text}\")\n",
    "                response.raise_for_status()\n",
    "                search_results = response.json()\n",
    "                \n",
    "                # Filter out results from excluded repositories\n",
    "                valid_items = []\n",
    "                excluded_count = 0\n",
    "                for item in search_results.get('items', []):\n",
    "                    repo_full_name = item.get('repository', {}).get('full_name', '')\n",
    "                    if not repo_full_name:\n",
    "                        continue\n",
    "                        \n",
    "                    should_exclude = False\n",
    "                    for excluded_repo in EXCLUDED_REPOS:\n",
    "                        if excluded_repo.endswith('/'):  # This is a user/org prefix\n",
    "                            if repo_full_name.startswith(excluded_repo):\n",
    "                                should_exclude = True\n",
    "                                break\n",
    "                        elif repo_full_name == excluded_repo:  # Exact match\n",
    "                            should_exclude = True\n",
    "                            break\n",
    "                    \n",
    "                    if not should_exclude:\n",
    "                        valid_items.append(item)\n",
    "                    else:\n",
    "                        excluded_count += 1\n",
    "                \n",
    "                valid_count = len(valid_items)\n",
    "                total_api_count = search_results.get('total_count', 0)\n",
    "                print(f\"GitHub search for {address}: API found {total_api_count}, Valid found: {valid_count}, Excluded: {excluded_count}\")\n",
    "                \n",
    "                return (valid_count > 0, valid_count)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                current_retry += 1\n",
    "                print(f\"Error searching GitHub (Attempt {current_retry}/{max_retries}): {str(e)}\")\n",
    "\n",
    "                if current_retry < max_retries:\n",
    "                    time.sleep(2 ** current_retry)  # Exponential backoff\n",
    "                    continue\n",
    "\n",
    "                return (False, 0)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from GitHub: {str(e)}\")\n",
    "                print(f\"Response text: {response.text}\")\n",
    "                return (False, 0)\n",
    "\n",
    "print(\"GitHubAPI class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContractAPI class defined.\n"
     ]
    }
   ],
   "source": [
    "class ContractAPI:\n",
    "    \"\"\"Handle API interactions with the contract explorer API.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_recent_transactions(contract_address: str, chain_id: int, limit: int = 5) -> List[Dict]:\n",
    "        \"\"\"Get the most recent transactions for a contract address.\"\"\"\n",
    "        url = f\"{CONTRACT_API_BASE_URL}/api/v1/public-contract/{chain_id}/address/{contract_address}/explorer/transactions\"\n",
    "        params = {\n",
    "            \"limit\": 5\n",
    "        }\n",
    "        \n",
    "        max_retries = 3\n",
    "        current_retry = 0\n",
    "        \n",
    "        while current_retry < max_retries:\n",
    "            try:\n",
    "                response = requests.get(\n",
    "                    url,\n",
    "                    headers=CONTRACT_API_HEADERS,\n",
    "                    params=params,\n",
    "                    verify=True,\n",
    "                    timeout=30\n",
    "                )\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    print(f\"Contract API Error (Transactions, {response.status_code}) for {contract_address} on chain {chain_id}: {response.text}\")\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                transactions = response.json()\n",
    "                # Ensure the response is a list\n",
    "                if not isinstance(transactions, list):\n",
    "                    print(f\"Warning: Unexpected response type for transactions (expected list, got {type(transactions)}). Data: {transactions}\")\n",
    "                    return []\n",
    "                \n",
    "                print(f\"Retrieved {len(transactions)} recent transactions for contract {contract_address} on chain {chain_id}\")\n",
    "                return transactions\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                current_retry += 1\n",
    "                print(f\"Error fetching transactions (Attempt {current_retry}/{max_retries}): {str(e)}\")\n",
    "                \n",
    "                if current_retry < max_retries:\n",
    "                    time.sleep(2 ** current_retry)  # Exponential backoff\n",
    "                    continue\n",
    "                \n",
    "                return []\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from Contract API (Transactions): {str(e)}\")\n",
    "                print(f\"Response text: {response.text}\")\n",
    "                return []\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_transaction_trace(tx_hash: str, chain_id: int) -> Optional[Dict]:\n",
    "        \"\"\"Get the trace for a specific transaction.\"\"\"\n",
    "        url = f\"{CONTRACT_API_BASE_URL}/api/v1/public-contract/{chain_id}/trace/{tx_hash}\"\n",
    "        \n",
    "        max_retries = 3\n",
    "        current_retry = 0\n",
    "        \n",
    "        while current_retry < max_retries:\n",
    "            try:\n",
    "                response = requests.get(\n",
    "                    url,\n",
    "                    headers=CONTRACT_API_HEADERS,\n",
    "                    verify=True,\n",
    "                    timeout=30\n",
    "                )\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    print(f\"Contract API Error (Trace, {response.status_code}) for tx {tx_hash} on chain {chain_id}: {response.text}\")\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                trace_data = response.json()\n",
    "                # Ensure the response is a dict\n",
    "                if not isinstance(trace_data, dict):\n",
    "                    print(f\"Warning: Unexpected response type for trace (expected dict, got {type(trace_data)}). Data: {trace_data}\")\n",
    "                    return None\n",
    "                \n",
    "                print(f\"Retrieved trace data for transaction {tx_hash} on chain {chain_id}\")\n",
    "                return trace_data\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                current_retry += 1\n",
    "                print(f\"Error fetching trace data for {tx_hash} (Attempt {current_retry}/{max_retries}): {str(e)}\")\n",
    "                \n",
    "                if current_retry < max_retries:\n",
    "                    time.sleep(2 ** current_retry)  # Exponential backoff\n",
    "                    continue\n",
    "                \n",
    "                return None\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from Contract API (Trace): {str(e)}\")\n",
    "                print(f\"Response text: {response.text}\")\n",
    "                return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_contract_additional_data(contract_address: str, chain: str) -> Optional[Dict]:\n",
    "        \"\"\"Process a contract by fetching recent transactions and their traces.\n",
    "        Returns a dictionary containing 'transactions' and 'traces' lists, or None if processing fails.\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            \"transactions\": [],\n",
    "            \"traces\": []\n",
    "        }\n",
    "        \n",
    "        # Get the chain ID from the chain name\n",
    "        chain_id = SUPPORTED_CHAINS.get(str(chain).lower())\n",
    "        if not chain_id:\n",
    "            print(f\"Unsupported or invalid chain: '{chain}'. Skipping additional API checks.\")\n",
    "            return None\n",
    "        \n",
    "        # Get recent transactions\n",
    "        transactions = ContractAPI.get_recent_transactions(contract_address, chain_id)\n",
    "        if not transactions:\n",
    "            print(f\"No transactions found or error fetching transactions for contract {contract_address} on chain {chain_id}\")\n",
    "            return None\n",
    "        \n",
    "        result[\"transactions\"] = transactions\n",
    "        \n",
    "        # Get trace data for each transaction\n",
    "        successful_traces = 0\n",
    "        for tx in transactions:\n",
    "            if isinstance(tx, dict) and 'hash' in tx:\n",
    "                tx_hash = tx.get(\"hash\")\n",
    "                if tx_hash and isinstance(tx_hash, str):\n",
    "                    trace = ContractAPI.get_transaction_trace(tx_hash, chain_id)\n",
    "                    if trace:\n",
    "                        result[\"traces\"].append({\n",
    "                            \"tx_hash\": tx_hash,\n",
    "                            \"trace_data\": trace\n",
    "                        })\n",
    "                        successful_traces += 1\n",
    "                    else:\n",
    "                        print(f\"Failed to retrieve trace for tx {tx_hash}\")\n",
    "                    # Add a small delay between API calls to avoid rate limiting\n",
    "                    time.sleep(0.5)\n",
    "                else:\n",
    "                    print(f\"Skipping trace fetch due to invalid or missing hash in transaction: {tx}\")\n",
    "            else:\n",
    "                print(f\"Skipping trace fetch due to invalid transaction format: {tx}\")\n",
    "                \n",
    "        print(f\"Successfully retrieved traces for {successful_traces}/{len(transactions)} transactions.\")\n",
    "        return result\n",
    "\n",
    "print(\"ContractAPI class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyperSyncAPI class defined with log fetching and network awareness.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "class HyperSyncAPI:\n",
    "    \"\"\"Handles API interactions with the Envio HyperSync API.\"\"\"\n",
    "\n",
    "    # --- Field Selections ---\n",
    "    # Adjust these lists based on the exact data fields you need from each table\n",
    "\n",
    "    TRANSACTION_FIELDS_FOR_RECENT = [\n",
    "        \"hash\", \"block_number\", \"transaction_index\", \"from\", \"to\", \"value\",\n",
    "        \"gas_used\", \"status\", \"input\", \"type\", \"gas_price\", \"effective_gas_price\"\n",
    "        # Add other needed fields like max_fee_per_gas etc.\n",
    "    ]\n",
    "    BLOCK_FIELDS_FOR_RECENT = [\"timestamp\", \"number\"] # Need timestamp for sorting\n",
    "\n",
    "    LOG_FIELDS = [\n",
    "        \"log_index\", \"transaction_index\", \"transaction_hash\", \"block_number\",\n",
    "        \"address\", \"data\",\n",
    "        \"topic0\", \"topic1\", \"topic2\", \"topic3\"\n",
    "    ]\n",
    "\n",
    "    TRACE_FIELDS = [\n",
    "        \"transaction_hash\", \"trace_address\", \"block_number\", \"from\", \"to\", \"value\",\n",
    "        \"gas\", \"gas_used\", \"input\", \"output\", \"type\", \"call_type\", \"error\"\n",
    "    ]\n",
    "\n",
    "    # --- Static Methods ---\n",
    "\n",
    "    @staticmethod\n",
    "    def _send_query(chain_id: int, query_payload: Dict) -> Optional[Dict]:\n",
    "        \"\"\"Sends a query to the HyperSync API for the specific chain and handles basic errors/retries.\"\"\"\n",
    "\n",
    "        # Get chain-specific config\n",
    "        chain_config = HYPERSYNC_CHAIN_CONFIG.get(chain_id)\n",
    "        if not chain_config:\n",
    "            print(f\"Error: HyperSync configuration not found for chain ID {chain_id}.\")\n",
    "            return None\n",
    "        \n",
    "        # Ensure URL ends with /query\n",
    "        base_url = chain_config[\"url\"]\n",
    "        url = base_url if base_url.endswith('/query') else f\"{base_url}/query\"\n",
    "\n",
    "        # Check API key configuration\n",
    "        if not url or not HYPERSYNC_HEADERS.get('Authorization') or 'None' in HYPERSYNC_HEADERS.get('Authorization', ''):\n",
    "             print(\"Error: HyperSync API URL or Key not configured.\")\n",
    "             return None\n",
    "\n",
    "        max_retries = 3\n",
    "        current_retry = 0\n",
    "\n",
    "        while current_retry < max_retries:\n",
    "            try:\n",
    "                print(f\"Sending HyperSync Query to {url} (Attempt {current_retry + 1})\")\n",
    "                # print(f\"Payload: {json.dumps(query_payload, indent=2)}\") # Uncomment for detailed payload logging\n",
    "                response = requests.post(\n",
    "                    url,\n",
    "                    headers=HYPERSYNC_HEADERS,\n",
    "                    json=query_payload,\n",
    "                    timeout=60 # Queries can take longer\n",
    "                )\n",
    "\n",
    "                # Handle Rate Limiting\n",
    "                if response.status_code == 429:\n",
    "                    print(f\"HyperSync rate limit hit. Retrying after delay...\")\n",
    "                    time.sleep(5 * (current_retry + 1))\n",
    "                    current_retry += 1\n",
    "                    continue\n",
    "\n",
    "                # Handle other errors\n",
    "                if response.status_code != 200:\n",
    "                     print(f\"HyperSync API Error ({response.status_code}): {response.text}\")\n",
    "                response.raise_for_status() # Raises HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "                # Decode JSON\n",
    "                data = response.json()\n",
    "                print(\"HyperSync Query successful.\")\n",
    "                # print(f\"HyperSync Response Snippet: {json.dumps(data, indent=2)[:500]}...\") # Optional\n",
    "                return data\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                current_retry += 1\n",
    "                print(f\"Error sending HyperSync query (Attempt {current_retry}/{max_retries}): {str(e)}\")\n",
    "                if current_retry < max_retries:\n",
    "                    time.sleep(2 ** current_retry) # Exponential backoff\n",
    "                    continue\n",
    "                return None # Failed after all retries\n",
    "            except json.JSONDecodeError as e:\n",
    "                 print(f\"Error decoding JSON from HyperSync: {str(e)}\")\n",
    "                 print(f\"Response text: {response.text}\")\n",
    "                 return None # Stop retrying on decode error\n",
    "\n",
    "        return None # Fallback if loop finishes unexpectedly\n",
    "\n",
    "    @staticmethod\n",
    "    def get_archive_height(chain_id: int) -> Optional[int]:\n",
    "        \"\"\"Gets the current archive height from HyperSync for a given chain.\"\"\"\n",
    "        height = None\n",
    "        try:\n",
    "            # --- Correction Start ---\n",
    "            # Get the chain configuration\n",
    "            chain_config = HYPERSYNC_CHAIN_CONFIG.get(chain_id)\n",
    "            if not chain_config:\n",
    "                print(f\"Error: No HyperSync configuration found for chain {chain_id}\")\n",
    "                return None\n",
    "            base_url = chain_config.get(\"url\")\n",
    "            if not base_url:\n",
    "                 print(f\"Error: No URL found in config for chain {chain_id}\")\n",
    "                 return None\n",
    "            # --- Correction End ---\n",
    "\n",
    "            # Attempt to hit the /height endpoint first\n",
    "            # Construct the /height URL correctly (remove /query if present)\n",
    "            height_url = base_url.replace('/query', '') + '/height'\n",
    "            print(f\"Attempting to fetch height from: {height_url}\")\n",
    "\n",
    "            response = requests.get(\n",
    "                height_url,\n",
    "                headers=HYPERSYNC_HEADERS, # Use the class headers\n",
    "                timeout=30\n",
    "            )\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                height_value = data.get('height')\n",
    "                if height_value is not None:\n",
    "                    height = int(height_value) # Store the height if found\n",
    "                    print(f\"HyperSync archive height for chain {chain_id} from /height endpoint: {height}\")\n",
    "                else:\n",
    "                    print(f\"Warning: /height endpoint for chain {chain_id} did not return 'height' field.\")\n",
    "            else:\n",
    "                print(f\"Warning: Failed to fetch from /height endpoint (HTTP {response.status_code}). Will attempt fallback.\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Warning: RequestException trying /height endpoint for chain {chain_id}: {e}. Will attempt fallback.\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Warning: JSONDecodeError trying /height endpoint for chain {chain_id}: {e}. Response: {response.text}. Will attempt fallback.\")\n",
    "        except Exception as e: # Catch other potential errors like int conversion\n",
    "             print(f\"Warning: Unexpected error trying /height endpoint for chain {chain_id}: {e}. Will attempt fallback.\")\n",
    "\n",
    "\n",
    "        # --- Fallback Logic ---\n",
    "        # If height wasn't successfully retrieved from the /height endpoint, use the query method\n",
    "        if height is None:\n",
    "            print(f\"Falling back to using /query endpoint to get archive height for chain {chain_id}...\")\n",
    "            query = {\"field_selection\": {}} # Empty query just to get metadata\n",
    "            response_data = HyperSyncAPI._send_query(chain_id, query) # Use the query method\n",
    "            if response_data:\n",
    "                height_value = response_data.get(\"archive_height\")\n",
    "                if height_value is not None:\n",
    "                     height = int(height_value)\n",
    "                     print(f\"HyperSync archive height for chain {chain_id} from /query fallback: {height}\")\n",
    "\n",
    "\n",
    "        # Final Check and Return\n",
    "        if height is None:\n",
    "             print(f\"Could not retrieve archive height for chain {chain_id} via /height or /query fallback.\")\n",
    "             return None\n",
    "        else:\n",
    "             return height\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_recent_transactions(contract_address: str, chain_id: int, limit: int = 5, block_range: int = 1000) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get the most recent transactions TO a specific contract address using HyperSync.\n",
    "        NOTE: Requires client-side sorting as HyperSync doesn't guarantee order beyond block range.\n",
    "        \"\"\"\n",
    "        print(f\"Fetching archive height for chain {chain_id} to determine query range...\")\n",
    "        archive_height = HyperSyncAPI.get_archive_height(chain_id)\n",
    "        if archive_height is None:\n",
    "            print(\"Could not determine archive height. Cannot fetch recent transactions.\")\n",
    "            return []\n",
    "\n",
    "        from_block = max(0, archive_height - block_range)\n",
    "        # Query up to and including the archive height\n",
    "        to_block = archive_height + 1\n",
    "\n",
    "        print(f\"Querying HyperSync on chain {chain_id} for transactions to {contract_address} in blocks [{from_block}, {to_block - 1}]...\")\n",
    "\n",
    "        query = {\n",
    "            \"from_block\": from_block,\n",
    "            \"to_block\": to_block,\n",
    "            \"transactions\": [{\n",
    "                # Ensure address is lowercase for consistent matching\n",
    "                \"to\": [contract_address.lower()]\n",
    "            }],\n",
    "            \"field_selection\": {\n",
    "                \"block\": HyperSyncAPI.BLOCK_FIELDS_FOR_RECENT,\n",
    "                \"transaction\": HyperSyncAPI.TRANSACTION_FIELDS_FOR_RECENT\n",
    "            },\n",
    "            \"join_mode\": \"JoinAll\" # Get associated block data (like timestamp)\n",
    "        }\n",
    "\n",
    "        response = HyperSyncAPI._send_query(chain_id, query)\n",
    "\n",
    "        if not response or 'data' not in response:\n",
    "            print(\"No data received from HyperSync or unexpected response format.\")\n",
    "            return []\n",
    "\n",
    "        # --- Start Correction ---\n",
    "        # Extract and combine block/transaction data efficiently\n",
    "        all_txs = []\n",
    "        # Create a lookup for block timestamps, ensuring items are dictionaries\n",
    "        block_timestamps = {}\n",
    "        for b in response['data'].get('blocks', []):\n",
    "            # Check if b is a dictionary AND has a 'number' before accessing\n",
    "            if isinstance(b, dict) and b.get('number') is not None:\n",
    "                block_timestamps[b.get('number')] = b.get('timestamp')\n",
    "            else:\n",
    "                # Log if an unexpected item is found in the blocks list\n",
    "                print(f\"Warning: Unexpected item found in 'blocks' list: {type(b)} - {b}\")\n",
    "        # --- End Correction ---\n",
    "\n",
    "        # The rest of the transaction processing loop remains the same\n",
    "        for tx in response['data'].get('transactions', []):\n",
    "            block_num = tx.get('block_number')\n",
    "            if block_num is not None:\n",
    "                tx_data = tx.copy() # Create a copy to modify\n",
    "                tx_data['timestamp'] = block_timestamps.get(block_num) # Add timestamp from block lookup\n",
    "                all_txs.append(tx_data)\n",
    "            else:\n",
    "                 print(f\"Warning: Transaction missing block_number: {tx.get('hash')}\")\n",
    "\n",
    "\n",
    "        # Sort client-side: Primary: block number (desc), Secondary: tx index (desc)\n",
    "        all_txs.sort(key=lambda x: (x.get('block_number', 0), x.get('transaction_index', 0)), reverse=True)\n",
    "\n",
    "        print(f\"Retrieved {len(all_txs)} transactions in range via HyperSync. Returning latest {limit}.\")\n",
    "        return all_txs[:limit]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_transaction_logs(tx_hash: str, chain_id: int, block_number: int) -> Optional[List[Dict]]:\n",
    "        \"\"\"\n",
    "        Get the logs for a specific transaction using HyperSync.\n",
    "        Requires the block_number where the transaction occurred.\n",
    "        Returns a list of log dictionaries (or None if API error, empty list if no logs found for TX).\n",
    "        \"\"\"\n",
    "        print(f\"Querying HyperSync on chain {chain_id} for logs of tx {tx_hash} in block {block_number}...\")\n",
    "\n",
    "        query = {\n",
    "            \"from_block\": block_number,\n",
    "            \"to_block\": block_number + 1, # Query only the specific block\n",
    "            \"include_all_blocks\": False, # Don't need block data unless specifically requested below\n",
    "            \"field_selection\": {\n",
    "                 \"log\": HyperSyncAPI.LOG_FIELDS,\n",
    "                 # \"block\": [\"number\", \"timestamp\"] # Optionally include block info if needed per log\n",
    "            },\n",
    "            \"join_mode\": \"Default\"\n",
    "        }\n",
    "        # Note: We fetch all logs for the block and filter client-side by transaction_hash.\n",
    "\n",
    "        response = HyperSyncAPI._send_query(chain_id, query)\n",
    "\n",
    "        # Check if response is valid and contains the 'logs' key in 'data'\n",
    "        if not response or 'data' not in response or 'logs' not in response['data']:\n",
    "            print(f\"No log data received from HyperSync or unexpected format for block {block_number}.\")\n",
    "            # Distinguish between API error (None) and valid response with no logs (empty list is handled below)\n",
    "            if response is None:\n",
    "                return None # API error occurred\n",
    "            else:\n",
    "                # Response received, but no 'logs' array within 'data'\n",
    "                print(f\"Response received, but no 'logs' field found in data for block {block_number}.\")\n",
    "                return []\n",
    "\n",
    "\n",
    "        # Filter logs client-side for the specific transaction hash\n",
    "        tx_logs = []\n",
    "        target_tx_hash_lower = tx_hash.lower()\n",
    "        for log in response['data']['logs']:\n",
    "            log_tx_hash = log.get('transaction_hash')\n",
    "            # Ensure log_tx_hash is not None before lowercasing\n",
    "            if log_tx_hash and log_tx_hash.lower() == target_tx_hash_lower:\n",
    "                tx_logs.append(log)\n",
    "\n",
    "        # Log outcome\n",
    "        if not tx_logs:\n",
    "            print(f\"No logs found specifically matching transaction hash {tx_hash} within block {block_number}.\")\n",
    "        else:\n",
    "            print(f\"Retrieved {len(tx_logs)} logs for transaction {tx_hash} via HyperSync.\")\n",
    "\n",
    "        # Return empty list if no logs found for this specific TX, or the list if found\n",
    "        return tx_logs\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_transaction_trace(tx_hash: str, chain_id: int, block_number: int) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Get the trace for a specific transaction using HyperSync.\n",
    "        Requires the block_number where the transaction occurred.\n",
    "        Returns a dictionary containing trace information (or None if error/not found/unsupported).\n",
    "        \"\"\"\n",
    "        # Check trace support for the chain first\n",
    "        chain_config = HYPERSYNC_CHAIN_CONFIG.get(chain_id)\n",
    "        if not chain_config:\n",
    "             print(f\"Error: HyperSync configuration not found for chain ID {chain_id}. Cannot fetch trace.\")\n",
    "             return None\n",
    "        if not chain_config.get(\"supports_traces\", False):\n",
    "            # Don't print warning here, let caller handle logic\n",
    "            # print(f\"Trace fetch skipped: Chain ID {chain_id} does not support traces.\")\n",
    "            return None # Indicate traces are unavailable/unsupported\n",
    "\n",
    "        print(f\"Querying HyperSync on chain {chain_id} for traces of tx {tx_hash} in block {block_number}...\")\n",
    "\n",
    "        query = {\n",
    "            \"from_block\": block_number,\n",
    "            \"to_block\": block_number + 1, # Query only the specific block\n",
    "            \"include_all_blocks\": False,\n",
    "            \"field_selection\": {\n",
    "                 \"trace\": HyperSyncAPI.TRACE_FIELDS,\n",
    "            },\n",
    "            \"join_mode\": \"Default\"\n",
    "        }\n",
    "        # Note: Fetching all traces for the block and filtering client-side by transaction_hash.\n",
    "\n",
    "        response = HyperSyncAPI._send_query(chain_id, query)\n",
    "\n",
    "        if not response or 'data' not in response or 'traces' not in response['data']:\n",
    "            print(f\"No trace data received from HyperSync or unexpected format for block {block_number}.\")\n",
    "            if response is None: return None # API error\n",
    "            else: return None # Treat missing traces in response as \"not found\" for this specific request\n",
    "\n",
    "        # Filter traces client-side for the specific transaction hash\n",
    "        tx_traces = []\n",
    "        target_tx_hash_lower = tx_hash.lower()\n",
    "        for trace in response['data']['traces']:\n",
    "            trace_tx_hash = trace.get('transaction_hash')\n",
    "            if trace_tx_hash and trace_tx_hash.lower() == target_tx_hash_lower:\n",
    "                tx_traces.append(trace)\n",
    "\n",
    "        if not tx_traces:\n",
    "            print(f\"No traces found specifically matching transaction hash {tx_hash} within block {block_number}.\")\n",
    "            return None # Return None if no traces found for this TX\n",
    "\n",
    "        print(f\"Retrieved {len(tx_traces)} trace entries for transaction {tx_hash} via HyperSync.\")\n",
    "        # Return traces wrapped in a dictionary for consistency\n",
    "        return {\"traces\": tx_traces}\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def process_contract_additional_data(contract_address: str, chain: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Process a contract using HyperSync: fetch recent transactions, their logs,\n",
    "        and optionally traces if supported by the network.\n",
    "        Returns a dictionary containing 'transactions', 'logs', and potentially 'traces',\n",
    "        or None if fetching recent transactions fails initially.\n",
    "        \"\"\"\n",
    "        # Prepare result structure\n",
    "        result = {\n",
    "            \"transactions\": [],\n",
    "            \"logs\": [], # Stores logs per transaction: {tx_hash: ..., log_data: [...]}\n",
    "            \"traces\": [] # Stores traces per transaction: {tx_hash: ..., trace_data: {...}}\n",
    "        }\n",
    "\n",
    "        # Validate chain and get ID\n",
    "        chain_id = SUPPORTED_CHAINS.get(str(chain).lower())\n",
    "        if not chain_id:\n",
    "            print(f\"Unsupported or invalid chain name: '{chain}'. Skipping HyperSync API checks.\")\n",
    "            return None # Cannot proceed without a valid/supported chain ID\n",
    "\n",
    "        # --- 1. Get recent transactions ---\n",
    "        transactions = HyperSyncAPI.get_recent_transactions(contract_address, chain_id)\n",
    "        # Critical Check: If we fail to get transactions, we cannot proceed.\n",
    "        if not transactions: # Handles both None (error) and empty list (no txs found)\n",
    "            print(f\"No recent transactions found via HyperSync for contract {contract_address} on chain {chain_id}. Cannot fetch logs/traces.\")\n",
    "            return None # Indicate failure or lack of data to process further\n",
    "\n",
    "        result[\"transactions\"] = transactions\n",
    "        print(f\"Found {len(transactions)} recent transactions to process for logs/traces.\")\n",
    "\n",
    "        # Determine if traces should be fetched for this chain\n",
    "        chain_config = HYPERSYNC_CHAIN_CONFIG.get(chain_id, {})\n",
    "        fetch_traces = chain_config.get(\"supports_traces\", False)\n",
    "        if fetch_traces:\n",
    "             print(f\"Chain {chain_id} supports traces. Will attempt to fetch them.\")\n",
    "        else:\n",
    "             print(f\"Chain {chain_id} does not support traces. Skipping trace fetch.\")\n",
    "\n",
    "        # --- 2. Get logs (and optionally traces) for each transaction ---\n",
    "        successful_logs_found = 0 # Count TXs for which we found >0 logs\n",
    "        successful_traces_found = 0 # Count TXs for which we found >0 trace entries\n",
    "\n",
    "        for tx in transactions:\n",
    "            tx_hash = tx.get(\"hash\")\n",
    "            block_num = tx.get(\"block_number\")\n",
    "\n",
    "            if tx_hash and block_num is not None:\n",
    "                # --- Fetch Logs ---\n",
    "                log_data = HyperSyncAPI.get_transaction_logs(tx_hash, chain_id, block_num)\n",
    "                # Check if log_data is None (API error) or a list (could be empty)\n",
    "                if log_data is not None:\n",
    "                    result[\"logs\"].append({\n",
    "                        \"tx_hash\": tx_hash,\n",
    "                        \"log_data\": log_data # Append the list of logs (could be empty)\n",
    "                    })\n",
    "                    if log_data: # Increment count only if the list wasn't empty\n",
    "                         successful_logs_found += 1\n",
    "                else:\n",
    "                    # Log API error if get_transaction_logs returned None\n",
    "                    print(f\"Failed to retrieve logs for tx {tx_hash} due to API error or timeout.\")\n",
    "\n",
    "                # --- Optionally Fetch Traces ---\n",
    "                if fetch_traces:\n",
    "                    trace_data = HyperSyncAPI.get_transaction_trace(tx_hash, chain_id, block_num)\n",
    "                    # Check if trace_data is None (error/unsupported/not found) or a dict\n",
    "                    if trace_data: # Contains {\"traces\": [...]}\n",
    "                        result[\"traces\"].append({\n",
    "                            \"tx_hash\": tx_hash,\n",
    "                            \"trace_data\": trace_data\n",
    "                        })\n",
    "                        successful_traces_found += 1\n",
    "                    # else: No need to print failure again, get_transaction_trace already did\n",
    "\n",
    "                # --- Delay ---\n",
    "                # Small delay to avoid overwhelming the API when processing multiple transactions\n",
    "                time.sleep(0.3) # Adjusted delay slightly\n",
    "            else:\n",
    "                print(f\"Skipping log/trace fetch for a transaction due to missing hash or block_number: {tx}\")\n",
    "\n",
    "        # --- Final Logging ---\n",
    "        print(f\"Finished processing additional data: Found logs for {successful_logs_found}/{len(transactions)} transactions.\")\n",
    "        if fetch_traces:\n",
    "            print(f\"Finished processing additional data: Found traces for {successful_traces_found}/{len(transactions)} transactions.\")\n",
    "\n",
    "        # Return the result dictionary (even if logs/traces arrays are empty for some TXs)\n",
    "        return result\n",
    "\n",
    "# --- End of Class Definition ---\n",
    "\n",
    "# Optional: Add a print statement after the class definition in your notebook cell\n",
    "print(\"HyperSyncAPI class defined with log fetching and network awareness.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_all_contracts function defined.\n"
     ]
    }
   ],
   "source": [
    "def process_all_contracts():\n",
    "    \"\"\"Process all unprocessed contracts from the target view.\"\"\"\n",
    "    processed_count = 0\n",
    "    updated_count = 0\n",
    "    failed_update_count = 0\n",
    "    github_not_found_count = 0\n",
    "    additional_api_checked_count = 0\n",
    "    \n",
    "    try:\n",
    "        # Fetch all unprocessed records directly from the view\n",
    "        print(f\"Fetching unprocessed records from Airtable view {TARGET_VIEW_ID}...\")\n",
    "        records = AirtableAPI.fetch_all_unprocessed_contracts()\n",
    "\n",
    "        if not records:\n",
    "            print(f\"No unprocessed records found in view {TARGET_VIEW_ID}.\")\n",
    "            return\n",
    "            \n",
    "        total_records = len(records)\n",
    "        print(f\"Found {total_records} unprocessed records to process.\")\n",
    "\n",
    "        # Process each record\n",
    "        for index, record in enumerate(records, 1):\n",
    "            processed_count += 1\n",
    "            record_id = record.get('id')\n",
    "            fields = record.get('fields', {})\n",
    "            contract_address = fields.get('address')\n",
    "            origin_key = fields.get('origin_key')\n",
    "            # Default to 'base' chain if missing\n",
    "            chain = fields.get('chain') or 'base'\n",
    "\n",
    "            if not record_id:\n",
    "                print(f\"Skipping record {index}/{total_records} - missing record ID\")\n",
    "                continue\n",
    "                \n",
    "            if not contract_address:\n",
    "                print(f\"Skipping record {record_id} ({index}/{total_records}) - no contract address\")\n",
    "                continue\n",
    "\n",
    "            print(f\"--- Processing {index}/{total_records}: Record ID {record_id}, Address {contract_address}, Chain {chain} ---\")\n",
    "\n",
    "            # Search GitHub\n",
    "            github_found, repo_count = GitHubAPI.search_contract(contract_address)\n",
    "\n",
    "            # If no GitHub repositories found, try the additional API\n",
    "            transaction_data = None\n",
    "            if not github_found:\n",
    "                github_not_found_count += 1\n",
    "                print(f\"No GitHub repositories found for {contract_address}, checking Contract API...\")\n",
    "                additional_api_checked_count += 1\n",
    "                transaction_data = ContractAPI.process_contract_additional_data(contract_address, chain)\n",
    "                \n",
    "                # Save to our tracking file regardless of transaction data success\n",
    "                AirtableAPI.save_no_github_contracts(contract_address, record_id, origin_key)\n",
    "            \n",
    "            # Update Airtable with github_found, repo_count, and transaction data if available\n",
    "            update_successful = AirtableAPI.update_record(record_id, github_found, repo_count, transaction_data)\n",
    "            if update_successful:\n",
    "                updated_count += 1\n",
    "            else:\n",
    "                failed_update_count += 1\n",
    "                print(f\"Failed to update Airtable record {record_id}\")\n",
    "\n",
    "            # Rate limiting delay between processing records\n",
    "            print(f\"--- Finished processing record {record_id}. Sleeping... ---\")\n",
    "            time.sleep(2)\n",
    "\n",
    "        print(f\"\\n=== Processing Summary ===\")\n",
    "        print(f\"Total records processed: {processed_count}\")\n",
    "        print(f\"Airtable records updated successfully: {updated_count}\")\n",
    "        print(f\"Airtable records failed to update: {failed_update_count}\")\n",
    "        print(f\"Contracts without GitHub presence: {github_not_found_count}\")\n",
    "        print(f\"Contracts checked via Contract API: {additional_api_checked_count}\")\n",
    "        print(f\"=========================\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during the process: {str(e)}\")\n",
    "\n",
    "print(\"process_all_contracts function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking structure of view: viwF2Xc24CGNO7u5C\n",
      "{\n",
      "  \"view_id\": \"viwF2Xc24CGNO7u5C\",\n",
      "  \"total_records\": 48,\n",
      "  \"field_names\": [\n",
      "    \"address\",\n",
      "    \"gas_eth\",\n",
      "    \"origin_key\",\n",
      "    \"avg_daa\",\n",
      "    \"contract_name\",\n",
      "    \"txcount\",\n",
      "    \"Blockexplorer\",\n",
      "    \"Github Search\",\n",
      "    \"Google\",\n",
      "    \"avg txcost\",\n",
      "    \"Dedaub\",\n",
      "    \"rel_cost\",\n",
      "    \"avg_success\",\n",
      "    \"is_proxy\",\n",
      "    \"day_range\",\n",
      "    \"Tenderly\",\n",
      "    \"block_explorer_url\"\n",
      "  ],\n",
      "  \"sample_record\": {\n",
      "    \"address\": \"0x83885CaB02a0b906836225442Fa17DcFE9cBf797\",\n",
      "    \"gas_eth\": 32.28007307967115,\n",
      "    \"origin_key\": [\n",
      "      \"recII4EbzHDPNgVY1\"\n",
      "    ],\n",
      "    \"avg_daa\": 7,\n",
      "    \"contract_name\": \"StrategyExecutor\",\n",
      "    \"txcount\": 332519,\n",
      "    \"Blockexplorer\": \"https://arbiscan.io/address/0x83885CaB02a0b906836225442Fa17DcFE9cBf797\",\n",
      "    \"Github Search\": \"https://github.com/search?q=0x83885CaB02a0b906836225442Fa17DcFE9cBf797&type=code\",\n",
      "    \"Google\": \"https://www.google.com/search?q=0x83885CaB02a0b906836225442Fa17DcFE9cBf797\",\n",
      "    \"avg txcost\": 9.707737927658614e-05,\n",
      "    \"Dedaub\": \"https://app.dedaub.com/arbitrum/address/0x83885CaB02a0b906836225442Fa17DcFE9cBf797/overview\",\n",
      "    \"rel_cost\": 1.5939936324535244,\n",
      "    \"avg_success\": 0.6695265134468956,\n",
      "    \"is_proxy\": true,\n",
      "    \"day_range\": 720,\n",
      "    \"Tenderly\": \"https://dashboard.tenderly.co/contract/arbitrum/0x83885CaB02a0b906836225442Fa17DcFE9cBf797/transactions\",\n",
      "    \"block_explorer_url\": [\n",
      "      \"https://arbiscan.io/\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Check Airtable View Structure\n",
    "if AIRTABLE_TOKEN:\n",
    "    print(f\"Checking structure of view: {TARGET_VIEW_ID}\")\n",
    "    view_info = AirtableAPI.get_view_structure()\n",
    "    print(json.dumps(view_info, indent=2))\n",
    "else:\n",
    "    print(\"Skipping Airtable view structure check (missing AIRTABLE_TOKEN).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching a batch of unprocessed contracts...\n",
      "Page 1: Fetched 48 records. Total: 48\n",
      "Total unprocessed records (NULL repo_count): 48\n",
      "Fetched 48 records. First record:\n",
      "{\n",
      "  \"id\": \"recwOBsxuN87BDpXV\",\n",
      "  \"createdTime\": \"2025-04-24T13:05:14.000Z\",\n",
      "  \"fields\": {\n",
      "    \"address\": \"0x83885CaB02a0b906836225442Fa17DcFE9cBf797\",\n",
      "    \"origin_key\": [\n",
      "      \"recII4EbzHDPNgVY1\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Fetch Unprocessed Contracts from Airtable\n",
    "if AIRTABLE_TOKEN:\n",
    "    print(\"Fetching a batch of unprocessed contracts...\")\n",
    "    unprocessed_records = AirtableAPI.fetch_all_unprocessed_contracts()\n",
    "    if unprocessed_records:\n",
    "        print(f\"Fetched {len(unprocessed_records)} records. First record:\")\n",
    "        print(json.dumps(unprocessed_records[0], indent=2))\n",
    "    else:\n",
    "        print(\"No unprocessed records found or error fetching.\")\n",
    "else:\n",
    "    print(\"Skipping Airtable fetch test (missing AIRTABLE_TOKEN).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching GitHub for address: 0x347cee5cC8C6FB4872123B40B799A8750f0E7EA2\n",
      "GitHub search for 0x347cee5cC8C6FB4872123B40B799A8750f0E7EA2: API found 1, Valid found: 0, Excluded: 1\n",
      "GitHub Search Result: Found=False, Valid Count=0\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Search GitHub for a Specific Contract Address\n",
    "test_address = \"0x347cee5cC8C6FB4872123B40B799A8750f0E7EA2\" # Example address\n",
    "if GITHUB_TOKEN:\n",
    "    print(f\"Searching GitHub for address: {test_address}\")\n",
    "    found, count = GitHubAPI.search_contract(test_address)\n",
    "    print(f\"GitHub Search Result: Found={found}, Valid Count={count}\")\n",
    "else:\n",
    "    print(\"Skipping GitHub search test (missing GITHUB_TOKEN).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching recent transactions for 0xA8C70657eBd7C9005A3Caa062B0e01308BE9c422 on chain base...\n",
      "Contract API Error (Transactions, 404) for 0xA8C70657eBd7C9005A3Caa062B0e01308BE9c422 on chain 8453: {\"error\":{\"id\":\"c35b2531-5234-42b4-ad72-9be9e20a17fd\",\"slug\":\"resource_not_found\",\"message\":\"Contract not found\",\"data\":{\"rid\":\"rid:contract\"}}}\n",
      "Error fetching transactions (Attempt 1/3): 404 Client Error: Not Found for url: https://api.tenderly.co/api/v1/public-contract/8453/address/0xA8C70657eBd7C9005A3Caa062B0e01308BE9c422/explorer/transactions?limit=5\n",
      "Contract API Error (Transactions, 404) for 0xA8C70657eBd7C9005A3Caa062B0e01308BE9c422 on chain 8453: {\"error\":{\"id\":\"a3904fd7-44cc-4a07-b2bb-b09f16dccc51\",\"slug\":\"resource_not_found\",\"message\":\"Contract not found\",\"data\":{\"rid\":\"rid:contract\"}}}\n",
      "Error fetching transactions (Attempt 2/3): 404 Client Error: Not Found for url: https://api.tenderly.co/api/v1/public-contract/8453/address/0xA8C70657eBd7C9005A3Caa062B0e01308BE9c422/explorer/transactions?limit=5\n",
      "Contract API Error (Transactions, 404) for 0xA8C70657eBd7C9005A3Caa062B0e01308BE9c422 on chain 8453: {\"error\":{\"id\":\"4210adc3-3ea6-4726-81b1-6765afd4a93d\",\"slug\":\"resource_not_found\",\"message\":\"Contract not found\",\"data\":{\"rid\":\"rid:contract\"}}}\n",
      "Error fetching transactions (Attempt 3/3): 404 Client Error: Not Found for url: https://api.tenderly.co/api/v1/public-contract/8453/address/0xA8C70657eBd7C9005A3Caa062B0e01308BE9c422/explorer/transactions?limit=5\n",
      "No transactions found or error fetching.\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Get Recent Transactions from Contract API\n",
    "test_tx_address = \"0xA8C70657eBd7C9005A3Caa062B0e01308BE9c422\" # Example address\n",
    "test_tx_chain = 'base' # Example chain \n",
    "\n",
    "if CONTRACT_API_BASE_URL != 'https://api.example.co':\n",
    "    print(f\"Fetching recent transactions for {test_tx_address} on chain {test_tx_chain}...\")\n",
    "    chain_id = SUPPORTED_CHAINS.get(test_tx_chain.lower())\n",
    "    if chain_id:\n",
    "        transactions = ContractAPI.get_recent_transactions(test_tx_address, chain_id)\n",
    "        if transactions:\n",
    "            print(f\"Found {len(transactions)} transactions. First transaction:\")\n",
    "            print(json.dumps(transactions[0], indent=2))\n",
    "        else:\n",
    "            print(\"No transactions found or error fetching.\")\n",
    "    else:\n",
    "        print(f\"Chain '{test_tx_chain}' is not supported.\")\n",
    "else:\n",
    "    print(\"Skipping Contract API transaction test (CONTRACT_API_BASE_URL not set).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching trace for transaction 0x94056f84cf1f9895560df8ced4ee712b36a1b46c6dde51a94be60b2cd0937736 on chain base...\n",
      "Retrieved trace data for transaction 0x94056f84cf1f9895560df8ced4ee712b36a1b46c6dde51a94be60b2cd0937736 on chain 8453\n",
      "Found trace data (showing specific keys):\n",
      "Method: fallback\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mlogs\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m trace_data:\n\u001b[32m     17\u001b[39m     event_names = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrace_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlogs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43mevent_names\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Test 5: Get Transaction Trace from Contract API\n",
    "test_trace_tx_hash = \"0x94056f84cf1f9895560df8ced4ee712b36a1b46c6dde51a94be60b2cd0937736\" # Example hash\n",
    "test_trace_chain = 'base' # Example chain\n",
    "\n",
    "if CONTRACT_API_BASE_URL != 'https://api.example.com':\n",
    "    print(f\"Fetching trace for transaction {test_trace_tx_hash} on chain {test_trace_chain}...\")\n",
    "    chain_id = SUPPORTED_CHAINS.get(test_trace_chain.lower())\n",
    "    if chain_id:\n",
    "        trace_data = ContractAPI.get_transaction_trace(test_trace_tx_hash, chain_id)\n",
    "        if trace_data:\n",
    "            print(f\"Found trace data (showing specific keys):\")\n",
    "            # Show method\n",
    "            if 'method' in trace_data:\n",
    "                print(f\"Method: {trace_data['method']}\")\n",
    "            # Extract and show unique event names from logs\n",
    "            if 'logs' in trace_data:\n",
    "                event_names = set()\n",
    "                for log in trace_data['logs']:\n",
    "                    if 'name' in log:\n",
    "                        event_names.add(log['name'])\n",
    "                print(f\"Event Names: {list(event_names)}\")\n",
    "            # Fallback if keys don't exist\n",
    "            if 'method' not in trace_data and 'logs' not in trace_data:\n",
    "                print(f\"Requested keys 'method' and 'logs' not found in trace data.\")\n",
    "                print(f\"Available keys: {list(trace_data.keys())}\")\n",
    "        else:\n",
    "            print(\"No trace data found or error fetching.\")\n",
    "    else:\n",
    "        print(f\"Chain '{test_trace_chain}' is not supported.\")\n",
    "else:\n",
    "    print(\"Skipping Contract API trace test (CONTRACT_API_BASE_URL not set).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 6: Process Additional Data for a Specific Contract\n",
    "test_proc_address = \"0x347cee5cc8c6fb4872123b40b799a8750f0e7ea2\" # Example address\n",
    "test_proc_chain = 'base' # Example chain\n",
    "\n",
    "if CONTRACT_API_BASE_URL != 'https://api.example.com':\n",
    "    print(f\"Processing additional data for {test_proc_address} on chain {test_proc_chain}...\")\n",
    "    additional_data = ContractAPI.process_contract_additional_data(test_proc_address, test_proc_chain)\n",
    "    if additional_data:\n",
    "        print(f\"Processed data: Found {len(additional_data.get('transactions', []))} transactions and {len(additional_data.get('traces', []))} traces.\")\n",
    "        # print(json.dumps(additional_data, indent=2)) # Uncomment to print full data\n",
    "    else:\n",
    "        print(\"No additional data processed (check logs for reasons).\")\n",
    "else:\n",
    "    print(\"Skipping Contract API processing test (CONTRACT_API_BASE_URL not set).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPERSYNC_API_URL: https://base.hypersync.xyz\n",
      "HYPERSYNC_API_KEY: *****f3ae\n",
      "\n",
      "Variable HYPERSYNC_API_URL: https://base.hypersync.xyz\n",
      "Variable HYPERSYNC_API_KEY: *****f3ae\n",
      "\n",
      "Condition Check:\n",
      "  HYPERSYNC_API_URL truthy? True\n",
      "  HYPERSYNC_API_KEY truthy? True\n",
      "  HYPERSYNC_API_URL != placeholder? True\n"
     ]
    }
   ],
   "source": [
    "# === Debug Cell: Check HyperSync Environment Variables ===\n",
    "\n",
    "print(f\"HYPERSYNC_API_URL: {os.getenv('HYPERSYNC_API_URL')}\")\n",
    "print(f\"HYPERSYNC_API_KEY: {'*' * 5 + os.getenv('HYPERSYNC_API_KEY')[-4:] if os.getenv('HYPERSYNC_API_KEY') else None}\") # Print partial key for verification\n",
    "\n",
    "# You can also check the values stored in the variables used by the code:\n",
    "print(f\"\\nVariable HYPERSYNC_API_URL: {HYPERSYNC_API_URL}\")\n",
    "print(f\"Variable HYPERSYNC_API_KEY: {'*' * 5 + HYPERSYNC_API_KEY[-4:] if HYPERSYNC_API_KEY else None}\")\n",
    "\n",
    "# Check the condition components:\n",
    "print(f\"\\nCondition Check:\")\n",
    "print(f\"  HYPERSYNC_API_URL truthy? {bool(HYPERSYNC_API_URL)}\")\n",
    "print(f\"  HYPERSYNC_API_KEY truthy? {bool(HYPERSYNC_API_KEY)}\")\n",
    "print(f\"  HYPERSYNC_API_URL != placeholder? {HYPERSYNC_API_URL != 'https://api.example.com'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting HyperSync Test for Contract: 0xA8C70657eBd7C9005A3Caa062B0e01308BE9c422 on Chain: base ---\n",
      "Fetching archive height for chain 8453 to determine query range...\n",
      "Attempting to fetch height from: https://base.hypersync.xyz/height\n",
      "HyperSync archive height for chain 8453 from /height endpoint: 29530340\n",
      "Querying HyperSync on chain 8453 for transactions to 0xA8C70657eBd7C9005A3Caa062B0e01308BE9c422 in blocks [29529340, 29530340]...\n",
      "Sending HyperSync Query to https://base.hypersync.xyz/query (Attempt 1)\n",
      "HyperSync Query successful.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m hypersync_configured = HYPERSYNC_API_URL \u001b[38;5;129;01mand\u001b[39;00m HYPERSYNC_API_KEY \u001b[38;5;129;01mand\u001b[39;00m HYPERSYNC_API_URL != \u001b[33m'\u001b[39m\u001b[33mhttps://api.example.com\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hypersync_configured:\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Call the main processing function for the specific contract\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     additional_data = \u001b[43mHyperSyncAPI\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_contract_additional_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_contract_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_chain_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# --- Output ---\u001b[39;00m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m additional_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 364\u001b[39m, in \u001b[36mHyperSyncAPI.process_contract_additional_data\u001b[39m\u001b[34m(contract_address, chain)\u001b[39m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# Cannot proceed without a valid/supported chain ID\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# --- 1. Get recent transactions ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m transactions = \u001b[43mHyperSyncAPI\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_recent_transactions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontract_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# Critical Check: If we fail to get transactions, we cannot proceed.\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transactions: \u001b[38;5;66;03m# Handles both None (error) and empty list (no txs found)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 208\u001b[39m, in \u001b[36mHyperSyncAPI.get_recent_transactions\u001b[39m\u001b[34m(contract_address, chain_id, limit, block_range)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m# Create a lookup for block timestamps, ensuring items are dictionaries\u001b[39;00m\n\u001b[32m    207\u001b[39m block_timestamps = {}\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mblocks\u001b[39m\u001b[33m'\u001b[39m, []):\n\u001b[32m    209\u001b[39m     \u001b[38;5;66;03m# Check if b is a dictionary AND has a 'number' before accessing\u001b[39;00m\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m b.get(\u001b[33m'\u001b[39m\u001b[33mnumber\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    211\u001b[39m         block_timestamps[b.get(\u001b[33m'\u001b[39m\u001b[33mnumber\u001b[39m\u001b[33m'\u001b[39m)] = b.get(\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# === Test Cell: Process Additional Data for a Specific Contract via HyperSync ===\n",
    "\n",
    "# --- Input ---\n",
    "test_contract_address = \"0xA8C70657eBd7C9005A3Caa062B0e01308BE9c422\" # Replace with your desired test address\n",
    "test_chain_name = 'base' # Replace with the desired chain name (e.g., 'ethereum', 'polygon')\n",
    "\n",
    "# --- Execution ---\n",
    "print(f\"--- Starting HyperSync Test for Contract: {test_contract_address} on Chain: {test_chain_name} ---\")\n",
    "\n",
    "# Check if HyperSync is configured\n",
    "hypersync_configured = HYPERSYNC_API_URL and HYPERSYNC_API_KEY and HYPERSYNC_API_URL != 'https://api.example.com'\n",
    "\n",
    "if hypersync_configured:\n",
    "    # Call the main processing function for the specific contract\n",
    "    additional_data = HyperSyncAPI.process_contract_additional_data(test_contract_address, test_chain_name)\n",
    "\n",
    "    # --- Output ---\n",
    "    if additional_data is not None:\n",
    "        print(\"\\n--- HyperSync Processing Results ---\")\n",
    "        \n",
    "        # Print summary counts\n",
    "        num_txs = len(additional_data.get('transactions', []))\n",
    "        logs_per_tx = additional_data.get('logs', [])\n",
    "        traces_per_tx = additional_data.get('traces', [])\n",
    "        num_logs_total = sum(len(item.get('log_data', [])) for item in logs_per_tx)\n",
    "        num_trace_entries_total = sum(len(item.get('trace_data', {}).get('traces', [])) for item in traces_per_tx)\n",
    "        \n",
    "        print(f\"Fetched Transactions: {num_txs}\")\n",
    "        print(f\"Total Logs Found Across Transactions: {num_logs_total}\")\n",
    "        print(f\"Total Trace Entries Found Across Transactions: {num_trace_entries_total}\")\n",
    "        \n",
    "        # Optionally print detailed data (can be very verbose)\n",
    "        print(\"\\n--- Details (Sample) ---\")\n",
    "        if additional_data['transactions']:\n",
    "             print(\"\\nSample Transaction:\")\n",
    "             print(json.dumps(additional_data['transactions'][0], indent=2))\n",
    "        else:\n",
    "             print(\"\\nNo transactions found.\")\n",
    "\n",
    "        if logs_per_tx:\n",
    "             # Find the first transaction that actually had logs\n",
    "             first_tx_with_logs = next((item for item in logs_per_tx if item.get('log_data')), None)\n",
    "             if first_tx_with_logs and first_tx_with_logs['log_data']:\n",
    "                   print(f\"\\nSample Log (from tx {first_tx_with_logs['tx_hash'][:10]}...):\")\n",
    "                   print(json.dumps(first_tx_with_logs['log_data'][0], indent=2))\n",
    "             else:\n",
    "                   print(\"\\nNo logs found for any fetched transaction.\")\n",
    "        else:\n",
    "             print(\"\\nLog data array is empty (check logs for fetch status).\")\n",
    "\n",
    "\n",
    "        if traces_per_tx:\n",
    "             # Find the first transaction that actually had trace data\n",
    "             first_tx_with_traces = next((item for item in traces_per_tx if item.get('trace_data', {}).get('traces')), None)\n",
    "             if first_tx_with_traces:\n",
    "                  print(f\"\\nSample Trace Entry (from tx {first_tx_with_traces['tx_hash'][:10]}...):\")\n",
    "                  print(json.dumps(first_tx_with_traces['trace_data']['traces'][0], indent=2))\n",
    "             else:\n",
    "                  print(\"\\nNo traces found for any fetched transaction (or network doesn't support traces).\")\n",
    "        else:\n",
    "             print(\"\\nTrace data array is empty (check logs for fetch status or network support).\")\n",
    "\n",
    "        # Uncomment below to print the entire structure (can be very large)\n",
    "        # print(\"\\n--- Full Result Structure ---\")\n",
    "        # print(json.dumps(additional_data, indent=2))\n",
    "\n",
    "    else:\n",
    "        print(\"\\n--- HyperSync Processing Failed ---\")\n",
    "        print(\"Failed to fetch initial transaction data or encountered an unrecoverable error.\")\n",
    "        print(\"Check previous logs for details (e.g., unsupported chain, API errors).\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n--- HyperSync Test Skipped ---\")\n",
    "    print(\"HyperSync API URL or Key is not configured. Please set HYPERSYNC_API_URL and HYPERSYNC_API_KEY in your environment or notebook.\")\n",
    "\n",
    "print(f\"\\n--- Finished HyperSync Test for Contract: {test_contract_address} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full process\n",
    "# Warning: This will modify your Airtable data for unprocessed records.\n",
    "\n",
    "run_full = False # Set to True to execute\n",
    "\n",
    "if run_full:\n",
    "    if AIRTABLE_TOKEN and GITHUB_TOKEN and CONTRACT_API_BASE_URL != 'https://api.example.com':\n",
    "        print(\"Starting full contract processing...\")\n",
    "        process_all_contracts()\n",
    "        print(\"Full contract processing finished.\")\n",
    "    else:\n",
    "        print(\"Cannot run full process. Check:\")\n",
    "        if not AIRTABLE_TOKEN: print(\"- Missing AIRTABLE_TOKEN\")\n",
    "        if not GITHUB_TOKEN: print(\"- Missing GITHUB_TOKEN\")\n",
    "        if CONTRACT_API_BASE_URL == 'https://api.example.com': print(\"- CONTRACT_API_BASE_URL is not set\")\n",
    "else:\n",
    "    print(\"Full process run skipped (run_full is False). Set run_full = True in the cell above to execute.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
